Read pasted-text.txtThought for 2sHere's the complete README.md content you can copy and paste directly:

```markdown
# Multimodal Fashion & Context Retrieval System  
### CLIP + FAISS + LLM + YOLO + SCHP + Places365

An end-to-end **ML-first**, **multimodal**, **region-aware fashion retrieval system** that goes beyond vanilla CLIP by explicitly modeling **compositional clothing attributes**, **scene context**, and **human body regions**.

---

## ğŸ”¥ Why This Project Exists

Most imageâ€“text retrieval systems using CLIP fail at **compositional queries**, for example:

- âŒ "red shirt with blue pants" vs "blue shirt with red pants"
- âŒ "white shirt with a red tie"
- âŒ "black jacket over a white inner shirt"
- âŒ "formal outfit inside an office"

This project **fixes that** by introducing **explicit spatial and semantic supervision** on top of CLIP embeddings.

---

## ğŸ§  Key Idea (In One Paragraph)

Instead of relying only on global CLIP embeddings, this system **decomposes a person image into semantic regions** (upper body, lower body, neck area), **extracts colors per region**, **classifies clothing layers**, **detects environment using Places365**, and **reranks results using structured logic driven by LLM-parsed queries**.

This makes the system **significantly better than vanilla CLIP** for fashion retrieval.

---

## ğŸ—ï¸ High-Level Architecture

```

User Query (Natural Language)
â†“
LLM + Rule-Based Query Parser
â†“
Structured Query Attributes
â†“
Semantic Retrieval (CLIP + FAISS)
â†“
Candidate Images
â†“
Attribute-Aware Reranking
â†“
Final Results + Explanations

```plaintext

---

## ğŸ“¦ Core Components

### 1ï¸âƒ£ Query Understanding (Text â†’ Structure)

**Hybrid Parsing Pipeline**
- Google Gemini (LLM)
- Rule-based NLP fallback
- Confidence-based switching

Extracted attributes:
- Upper garment type
- Lower garment type
- Neck/tie presence
- Colors per region
- Environment
- Style / vibe

---

### 2ï¸âƒ£ Vision Processing (Image â†’ Structure)

Each image goes through **five ML stages**:

| Stage | Model | Purpose |
|------|------|--------|
| Person Detection | YOLOv8 | Crop human region |
| Human Parsing | SCHP (LIP) | Pixel-level clothing regions |
| Region Segmentation | SCHP masks | Upper / Lower / Neck |
| Color Extraction | KMeans on crops | Region-specific colors |
| Scene Classification | Places365 | Indoor / Outdoor / Runway / Park |

---

### 3ï¸âƒ£ Semantic Retrieval

- **CLIP ViT-B/32**
- **FAISS IVFFlat index**
- 512-dim normalized embeddings
- Over-fetch + rerank strategy

---

### 4ï¸âƒ£ Reranking (Where the Magic Happens)

Final ranking is **not CLIP-only**.

We score based on:
- Upper garment color match
- Lower garment color match
- Neck/tie color match
- Garment type consistency
- Scene alignment
- Style / vibe compatibility

Each result also includes a **natural-language explanation**.

---

## ğŸ“‚ Project Directory Structure

```

fashion-context-search/
â”‚
â”œâ”€â”€ backend/
â”‚ â”œâ”€â”€ api/
â”‚ â”‚ â”œâ”€â”€ main.py
â”‚ â”‚ â”œâ”€â”€ routes.py
â”‚ â”‚ â””â”€â”€ schemas.py
â”‚ â”‚
â”‚ â”œâ”€â”€ indexer/
â”‚ â”‚ â”œâ”€â”€ build_index.py
â”‚ â”‚ â”œâ”€â”€ region_extractor.py
â”‚ â”‚ â”œâ”€â”€ color_extractor.py
â”‚ â”‚ â”œâ”€â”€ clothing_extractor.py
â”‚ â”‚ â”œâ”€â”€ vibe_extractor.py
â”‚ â”‚ â”œâ”€â”€ environment_extractor.py
â”‚ â”‚ â”œâ”€â”€ tie_extractor.py
â”‚ â”‚ â””â”€â”€ clip_zeroshot.py
â”‚ â”‚
â”‚ â”œâ”€â”€ models/
â”‚ â”‚ â”œâ”€â”€ clip_loader.py
â”‚ â”‚ â”œâ”€â”€ places365_loader.py
â”‚ â”‚ â”œâ”€â”€ scene_loader.py
â”‚ â”‚ â””â”€â”€ attribute_head.py
â”‚ â”‚
â”‚ â”œâ”€â”€ parsing/
â”‚ â”‚ â””â”€â”€ schp_parser.py
â”‚ â”‚
â”‚ â”œâ”€â”€ retrieval/
â”‚ â”‚ â”œâ”€â”€ search.py
â”‚ â”‚ â”œâ”€â”€ reranker.py
â”‚ â”‚ â”œâ”€â”€ query_parser.py
â”‚ â”‚ â”œâ”€â”€ llm_parser.py
â”‚ â”‚ â”œâ”€â”€ rule_parser.py
â”‚ â”‚ â”œâ”€â”€ confidence.py
â”‚ â”‚ â””â”€â”€ test_retrieval.py
â”‚ â”‚
â”‚ â””â”€â”€ vector_store/
â”‚ â””â”€â”€ faiss_store.py
â”‚
â”œâ”€â”€ checkpoints/
â”‚ â”œâ”€â”€ attribute_head.pt
â”‚ â”œâ”€â”€ places365_resnet18.pth
â”‚ â””â”€â”€ categories_places365.txt
â”‚
â”œâ”€â”€ external/
â”‚ â””â”€â”€ SCHP/
â”‚ â”œâ”€â”€ networks/
â”‚ â”œâ”€â”€ modules/
â”‚ â”œâ”€â”€ datasets/
â”‚ â”œâ”€â”€ utils/
â”‚ â”œâ”€â”€ simple_extractor.py
â”‚ â””â”€â”€ train.py
â”‚
â”œâ”€â”€ data/
â”‚ â”œâ”€â”€ raw/
â”‚ â”œâ”€â”€ processed/
â”‚ â”‚ â””â”€â”€ faiss_index/
â”‚ â”‚ â”œâ”€â”€ index.faiss
â”‚ â”‚ â””â”€â”€ metadata.json
â”‚ â””â”€â”€ metadata/
â”‚
â”œâ”€â”€ frontend/
â”‚ â””â”€â”€ app.py
â”‚
â”œâ”€â”€ notebooks/
â”‚ â”œâ”€â”€ 01_dataset_preparation.ipynb
â”‚ â””â”€â”€ 02_attribute_analysis.ipynb
â”‚
â”œâ”€â”€ scripts/
â”‚ â””â”€â”€ reduce_dataset.py
â”‚
â”œâ”€â”€ model_cache/
â”‚
â”œâ”€â”€ .env
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ README.md
â””â”€â”€ WINDOWS_SETUP_GUIDE.md

```plaintext

---

## ğŸ§ª Dataset

- Source: Fashionpedia + curated runway / street datasets
- Size: 1,000 images (configurable)
- Diversity:
  - Runway
  - Street
  - Park
  - Office
  - Casual / Formal / Editorial

---

## ğŸ”¬ Indexing Pipeline (Part A)

### What Happens During Indexing

For **each image**:

1. YOLO detects person
2. SCHP produces segmentation mask
3. Upper / lower / neck masks extracted
4. Region-wise color extraction
5. CLIP image embedding computed
6. Places365 predicts scene
7. Metadata stored in FAISS

### Command

```bash
python -m backend.indexer.build_index \
  --image_dir data/raw \
  --output_dir data/processed/faiss_index
```

---

## Retrieval Pipeline (Part B)

### Example Query

"A white shirt with a red tie in a formal office setting"

### Parsed Output

```json
{
  "upper_item": "shirt",
  "upper_colors": ["white"],
  "neck_item": "tie",
  "neck_colors": ["red"],
  "environment": "office",
  "vibe": "business_formal",
  "confidence": 0.92
}
```

### Why This Works Better Than CLIP Alone

| Vanilla CLIP | This System
|-----|-----
| Global embedding | Region-aware
| No compositionality | Explicit garment roles
| No scene logic | Places365
| No explanation | Human-readable reasoning


---

## ï¸ Frontend (Streamlit)

- Chat-style UI
- Attribute visualization
- Confidence indicators
- Explanations per result
- Designed for demo + evaluation


---

## ï¸ Installation (Windows)

### 1. Create Environment

```shellscript
python -m venv venv
venv\Scripts\activate
```

### 2. Install Dependencies

```shellscript
pip install -r requirements.txt
pip install ninja
```

### 3. Environment Variables

```plaintext
GOOGLE_API_KEY=your_gemini_key
```

---

## Running the System

### Backend

```shellscript
uvicorn backend.api.main:app --reload
```

### Frontend

```shellscript
streamlit run frontend/app.py
```

---

## Evaluation Queries (Assignment)

| Query | Supported
|-----|-----
| Yellow raincoat | âœ…
| Business attire in office | âœ…
| Blue shirt on park bench | âœ…
| Casual city walk | âœ…
| Red tie + white shirt | âœ…
| Blue shirt + black pants | âœ…


---

## Scalability

- FAISS IVFFlat scales to 1M+ images
- Index sharding supported
- Embeddings reusable
- Parsing models frozen


---

## ML-Centric Design Decisions

- Avoided overengineering infra
- Focused on attribute reasoning
- Used pretrained, proven models
- Explicit compositional handling


---

## Files You Can Safely Delete (Cleanup)

### Optional / Junk (After Final Submission)

- notebooks/
- scripts/
- training/
- model_cache/
- **pycache**/
- steps.txt
- QUICKSTART.txt
- package.json


### Do NOT Delete

- external/SCHP/
- checkpoints/
- backend/
- frontend/
- data/processed/


---

## Known Limitations

- SCHP is CPU-heavy
- No fine-grained fabric textures yet
- No multi-person disambiguation
- No temporal reasoning


---

## Future Work

- Lightweight parsing model
- Faster human parsing
- Fabric / pattern classification
- Multi-person queries
- Weather-aware outfits


---

## License

MIT License

---

## Final Note

This project is intentionally ML-heavy, not infra-heavy.

It demonstrates:

- Multimodal reasoning
- Compositional understanding
- Practical ML system design
- Clear extensibility


